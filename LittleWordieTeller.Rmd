---
title: "Little Wordie-Teller"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    includes:
      after_body: footer.html
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
# set option/configuration
knitr::opts_chunk$set(cache = TRUE)

# Load libraries
library(flexdashboard)
library(shiny)
library(data.table)
library(stringr)
```

```{r data,include=FALSE}
# load data
unigrams <- as.data.table(readRDS("unigramsSS.RData"))
bigrams <- as.data.table(readRDS("bigramsSS.RData"))
trigrams <- as.data.table(readRDS("trigramsSS.RData"))
quadgrams <- as.data.table(readRDS("quadgramsSS.RData"))
``` 

```{r context="server"}
word.predictor <- function(text.typed){
   
text <- unlist(str_extract_all(tolower(text.typed) ,"[a-z]+'?[a-z]*"))

n.words <- length(text)

if (n.words>=3){text <- text[(n.words-2):n.words]}
else if(n.words==2){text <- c(NA,text)}
else {text <- c(NA,NA,text)}

prediction <- quadgrams %>% subset(unigram==text[1] & bigram==text[2] & trigram==text[3], select=c(1,5))
        
if(is.na(prediction[1,]$quadgram)) {
                prediction <-  trigrams %>%  subset(unigram==text[2] & bigram==text[3], select=c(1,4))
                
                if(is.na(prediction[1,]$trigram)) {
                        prediction <- bigrams %>% subset(unigram==text[3], select=c(1,3))}
                     else {
                             prediction <- as.character(sample(unigrams[6:100,]$unigram,1)) # sample from top 100 unigrams excluding 'the' 'to' 'and' 'of' 'a'                            
}}       
if(is.character(prediction)) {
        cat(prediction,"\n")
}
       else{cat(as.character(unlist(prediction[1:nrow(prediction),][,1]))[1],"\n")}}

```


Prediction {.sidebar data-width=285}
=====================================

<b>Little Wordie-Teller</b> is a lightweight App that allows you to enter a sequence of English words and predict the next possible word towards completing your stream of thoughts.  

The App is designed with the assumption that you can predict the occurrence of a word from just a few words that preceed it.

Enjoy typing and predicting! 


Prediction {.page data-orientation=rows}
=====================================

<b>Little Wordie-Teller</b> works well when you enter at least 4 English words. Of course if you enter only one, it will also give you the best guess.

```{r}
textInput("Userwords", h4("Enter your words here:"), value = "", width = "99%", placeholder = "Please type here")
```

```{r context ="server"}
prediction <- reactive({ 
        text <- input$Userwords
        prediction <- word.predictor(text)
        }) 

output$wordie <- renderPrint({
        prediction()
        })


```

<hr>

<p><b>Little Wordie-Teller</b> says your next word can be:</p>

```{r echo=FALSE, context="render"}

div(style="width:500px;padding-left:100px;text-align:center;",fluidRow(verbatimTextOutput("wordie")))
 
```

Note: when it shows NA, it means you haven't entered any text. When it is in grey color, it means the prediction algorithm is still running. please be patient. 

<hr>

You can end up with these kinds of sentences as Little Wordie-Teller predicts along:

1) this is going to be a part of the reason why I think it is a very good job. 

2) wonder what first time it was a good idea to have a good time.

3) i don't know if i can get a little more time to work on the new jersey state police and the u.s government

4) last year due to the lack of a better way to celebrate the new year with a goal of mine


About the App {.page data-orientation=rows}
===================================== 

<b>Little Wordie-Teller</b> has been designed to fulfill the <b>Coursera Data Science Capstone project</b> provided by JHU.

This Shiny App is based on Markov ngram model which assumes the next word you are going to type can be predicted from its previous few words. A large corpus of textual data consisting of blogs, news and tweets are used to build 1,2,3,4-grams frequency tables. 

The **modified Kneser-Ney smoothing** technique (Chen & Goodman,1999) has been implemented to obtain probabilities of those ngrams. This technique is tested to be the best-performance version of Kneser-Ney smoothing. Instead of using a fixed discounting value of 0.75, the modiÔ¨Åed Kneser-Ney uses three different discounts for N-grams with counts of 1, 2 and three or more, respectively. I have thus obtained tables of Ngrams with KN probabilities which provide the base for my prediction algorithm. 

A simple backoff strategy is used to choose the next word. If the input is not found in the training data, a unigram is sampled to be the "best guess". It is due to this sampling nature that **Little Wordie-Teller** is a bit temperamental. The App works best when you type a sequence of words that make sense. Although accuracy of prediction can be low, it is fun at times to see what comes next!

For more details about code of the project, please see my github page [here](https://github.com/Angashley/coursera-capstone-project). 

